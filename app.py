import streamlit as st
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import pandas as pd
import time

st.set_page_config(page_title="TDnetã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¤œç´¢(æœˆæ›œå¤œãƒ»æœ€å¼·ç‰ˆ)", layout="wide")
st.title("ğŸ” TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¨ªæ–­æ¤œç´¢ãƒ„ãƒ¼ãƒ«")

with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    keyword = st.text_input("æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value="å¢—ç”£")
    # æœˆæ›œå¤œãªã®ã§ã€æ€ã„åˆ‡ã£ã¦200ä»¶ãã‚‰ã„ã‚¹ã‚­ãƒ£ãƒ³ã—ã¾ã—ã‚‡ã†
    search_limit = st.slider("ã‚¹ã‚­ãƒ£ãƒ³ä»¶æ•°", 10, 300, 100)
    search_button = st.button("æ¤œç´¢å®Ÿè¡Œ")

@st.cache_data(ttl=300)
def get_tdnet_list():
    url = "https://www.release.tdnet.info/inbs/I_main_00.html"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        res.encoding = res.apparent_encoding
        soup = BeautifulSoup(res.text, "html.parser")
        items = []
        rows = soup.select("#main-list-table tr")
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 5: continue
            title_tag = cols[3].find("a")
            if title_tag:
                items.append({
                    "æ™‚åˆ»": cols[0].text.strip(),
                    "ã‚³ãƒ¼ãƒ‰": cols[1].text.strip(),
                    "ç¤¾å": cols[2].text.strip(),
                    "ã‚¿ã‚¤ãƒˆãƒ«": title_tag.text.strip(),
                    "URL": "https://www.release.tdnet.info/inbs/" + title_tag.get("href")
                })
        return items
    except:
        return []

def search_in_pdf(url, kw):
    try:
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’çŸ­ãã—ã¦ã€ãƒ€ãƒ¡ãªPDFã¯ã™ãé£›ã°ã™
        response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=5)
        if response.status_code != 200: return None
        
        with pdfplumber.open(io.BytesIO(response.content)) as pdf:
            # 1ãƒšãƒ¼ã‚¸ç›®ã ã‘ã§ã‚‚ã€Œã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã€ãŒã‚ã‚Œã°å³ãƒ’ãƒƒãƒˆã«ã™ã‚‹ï¼ˆé«˜é€ŸåŒ–ï¼‰
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if text and kw in text:
                    return i + 1
                if i > 5: break # 6ãƒšãƒ¼ã‚¸ç›®ä»¥é™ã¯è¦‹ãªã„ï¼ˆæ±ºç®—çŸ­ä¿¡ã®ãƒ¡ã‚¤ãƒ³ã¯æœ€åˆã®æ–¹ãªã®ã§ï¼‰
    except:
        pass
    return None

if search_button:
    all_items = get_tdnet_list()
    if not all_items:
        st.error("TDnetã‹ã‚‰ãƒªã‚¹ãƒˆã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    else:
        target = all_items[:search_limit]
        st.info(f"æœˆæ›œæ—¥ã®æ–°ç€ {len(target)} ä»¶ã‚’ã‚¹ã‚­ãƒ£ãƒ³ä¸­... ã€Œ{keyword}ã€ã‚’æ¢ã—ã¦ã„ã¾ã™ã€‚")
        
        progress = st.progress(0)
        results = []
        status = st.empty()
        
        for idx, item in enumerate(target):
            progress.progress((idx + 1) / len(target))
            status.text(f"ã€{idx+1}/{len(target)}ã€‘ èª¿æŸ»ä¸­: {item['ç¤¾å']}")
            
            p = search_in_pdf(item["URL"], keyword)
            if p:
                item["ãƒšãƒ¼ã‚¸"] = p
                results.append(item)
                # ãƒ’ãƒƒãƒˆã—ãŸã‚‰ãã®å ´ã§è¡¨ç¤ºï¼ˆãƒ¢ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ç¶­æŒï¼ï¼‰
                st.toast(f"çš„ä¸­ï¼: {item['ç¤¾å']}")
            
            # é€£ç¶šã‚¢ã‚¯ã‚»ã‚¹ã§ãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œãªã„ã‚ˆã†ã€ã‚ãšã‹ã«å¾…æ©Ÿ
            time.sleep(0.05)
        
        status.empty()
        if results:
            st.success(f"ãŠå®ç™ºè¦‹ï¼ {len(results)} ä»¶ãƒ’ãƒƒãƒˆã—ã¾ã—ãŸã€‚")
            df = pd.DataFrame(results)
            st.dataframe(df, column_config={"URL": st.column_config.LinkColumn()})
        else:
            st.warning(f"ã€Œ{keyword}ã€ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’ã€ä¿®æ­£ã€ã‚„ã€é…å½“ã€ã«å¤‰ãˆã¦ã¿ã¦ãã ã•ã„ã€‚")
