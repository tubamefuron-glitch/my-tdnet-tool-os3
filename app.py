import streamlit as st
import requests
from bs4 import BeautifulSoup
import pdfplumber
import io
import pandas as pd

st.set_page_config(page_title="TDnetæ¨ªæ–­æ¤œç´¢ãƒ„ãƒ¼ãƒ«", layout="wide")
st.title("ğŸ” TDnet PDFã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æ¨ªæ–­æ¤œç´¢ãƒ„ãƒ¼ãƒ«")
st.caption("å°é‡å’Œå½¦æ°ã®ãƒ„ãƒ¼ãƒ«ã‚’å‚è€ƒã«ä½œæˆã—ãŸãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼è¨­å®š
with st.sidebar:
    st.header("æ¤œç´¢æ¡ä»¶")
    keyword = st.text_input("æ¤œç´¢ã™ã‚‹ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰", value="å¢—ç”£")
    st.info("ä¸€åº¦ã«ãƒã‚§ãƒƒã‚¯ã™ã‚‹ä»¶æ•°ãŒå¤šã„ã¨æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚ã¾ãšã¯å°‘é‡ã§ãƒ†ã‚¹ãƒˆã—ã¦ãã ã•ã„ã€‚")
    search_limit = st.slider("ãƒã‚§ãƒƒã‚¯ä»¶æ•°ï¼ˆæ–°ç€é †ï¼‰", 10, 100, 30)
    search_button = st.button("æ¤œç´¢å®Ÿè¡Œ")

@st.cache_data(ttl=300)
def get_tdnet_list():
    url = "https://www.release.tdnet.info/inbs/I_main_00.html"
    res = requests.get(url)
    res.encoding = res.apparent_encoding
    soup = BeautifulSoup(res.text, "html.parser")
    items = []
    table = soup.find("table", id="main-list-table")
    if not table: return []
    for row in table.find_all("tr")[1:]:
        cols = row.find_all("td")
        if len(cols) < 5: continue
        title_tag = cols[3].find("a")
        if title_tag:
            items.append({
                "æ™‚åˆ»": cols[0].text.strip(),
                "ã‚³ãƒ¼ãƒ‰": cols[1].text.strip(),
                "ç¤¾å": cols[2].text.strip(),
                "ã‚¿ã‚¤ãƒˆãƒ«": title_tag.text.strip(),
                "URL": "https://www.release.tdnet.info/inbs/" + title_tag.get("href")
            })
    return items

def search_in_pdf(url, kw):
    try:
        response = requests.get(url, timeout=10)
        with pdfplumber.open(io.BytesIO(response.content)) as pdf:
            for i, page in enumerate(pdf.pages):
                text = page.extract_text()
                if text and kw in text:
                    return i + 1
    except:
        pass
    return None

if search_button:
    all_items = get_tdnet_list()
    target_items = all_items[:search_limit]
    
    st.write(f"ç›´è¿‘ {len(target_items)} ä»¶ã®é–‹ç¤ºè³‡æ–™å†…ã‚’ã€Œ{keyword}ã€ã§ã‚¹ã‚­ãƒ£ãƒ³ä¸­...")
    progress_bar = st.progress(0)
    results = []
    
    for idx, item in enumerate(target_items):
        progress_bar.progress((idx + 1) / len(target_items))
        page_found = search_in_pdf(item["URL"], keyword)
        if page_found:
            item["ãƒ’ãƒƒãƒˆãƒšãƒ¼ã‚¸"] = page_found
            results.append(item)
    
    if results:
        st.success(f"ã€çš„ä¸­ã€‘ã€Œ{keyword}ã€ã‚’å«ã‚€è³‡æ–™ãŒ {len(results)} ä»¶è¦‹ã¤ã‹ã‚Šã¾ã—ãŸã€‚")
        df = pd.DataFrame(results)
        st.data_editor(df, column_config={"URL": st.column_config.LinkColumn()})
    else:
        st.warning(f"ã€Œ{keyword}ã€ã‚’å«ã‚€è³‡æ–™ã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚ä»¶æ•°ã‚’å¢—ã‚„ã™ã‹ã€åˆ¥ã®ãƒ¯ãƒ¼ãƒ‰ã§è©¦ã—ã¦ãã ã•ã„ã€‚")
